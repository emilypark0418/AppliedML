{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d9bcdbd2-3401-41ad-a83f-830e9346e607",
      "metadata": {
        "id": "d9bcdbd2-3401-41ad-a83f-830e9346e607"
      },
      "source": [
        "# **Applied Machine Learning Homework 5**\n",
        "**Due 2 May,2022 (Monday) 11:59PM EST**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70df26be-5638-4b0d-a252-4437eb76aa46",
      "metadata": {
        "id": "70df26be-5638-4b0d-a252-4437eb76aa46"
      },
      "source": [
        "### Natural Language Processing\n",
        "We will train a supervised training model to predict if a tweet has a positive or negative sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e0d9a19-25ea-4490-b0e8-7909bcdc3d9d",
      "metadata": {
        "id": "2e0d9a19-25ea-4490-b0e8-7909bcdc3d9d"
      },
      "source": [
        "####  **Dataset loading & dev/test splits**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fafa37c4-c8fc-4697-9bbe-11539d710bf7",
      "metadata": {
        "id": "fafa37c4-c8fc-4697-9bbe-11539d710bf7"
      },
      "source": [
        "**1.1) Load the twitter dataset from NLTK library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f4ce405-237b-42d2-9c81-25ff28deaf4a",
      "metadata": {
        "id": "5f4ce405-237b-42d2-9c81-25ff28deaf4a",
        "outputId": "1ddd96c2-a8e1-43dd-f17e-f5457fbc3bd9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to\n",
            "[nltk_data]     C:\\Users\\cwp94\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "from nltk.corpus import twitter_samples "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c41d62ce-3c78-4b6c-9238-111d990d170f",
      "metadata": {
        "id": "c41d62ce-3c78-4b6c-9238-111d990d170f"
      },
      "source": [
        "**1.2) Load the positive & negative tweets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b92fb408-f72a-4c23-acd8-7c944a52edd3",
      "metadata": {
        "id": "b92fb408-f72a-4c23-acd8-7c944a52edd3"
      },
      "outputs": [],
      "source": [
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12eae071-fd8a-4a46-9958-0525c635fd88",
      "metadata": {
        "id": "12eae071-fd8a-4a46-9958-0525c635fd88"
      },
      "source": [
        "**1.3) Create a development & test split (80/20 ratio):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f3673db-d7a8-470b-a3d3-f4522cd359b8",
      "metadata": {
        "id": "0f3673db-d7a8-470b-a3d3-f4522cd359b8",
        "outputId": "831da054-9175-4db7-b8f3-75743a649441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size of positive tweets: 5000\n",
            "size of negative tweets: 5000\n"
          ]
        }
      ],
      "source": [
        "#code here\n",
        "from sklearn.model_selection import train_test_split\n",
        "print(\"size of positive tweets:\", len(all_positive_tweets))\n",
        "print(\"size of negative tweets:\", len(all_negative_tweets))\n",
        "\n",
        "pos_dev, pos_test, neg_dev, neg_test = train_test_split(all_positive_tweets, all_negative_tweets, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1119a571",
      "metadata": {
        "id": "1119a571"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "## dev\n",
        "pos_dev = np.array(pos_dev)\n",
        "neg_dev = np.array(neg_dev)\n",
        "X_dev = np.concatenate((pos_dev, neg_dev), axis=0)\n",
        "\n",
        "pos_label_dev = np.ones((pos_dev.shape[0],1))\n",
        "neg_label_dev = np.zeros((neg_dev.shape[0],1))\n",
        "y_dev = np.concatenate((pos_label_dev, neg_label_dev), axis=0)\n",
        "\n",
        "## test\n",
        "pos_test = np.array(pos_test)\n",
        "neg_test = np.array(neg_test)\n",
        "X_test = np.concatenate((pos_test, neg_test), axis=0)\n",
        "\n",
        "pos_label_test = np.ones((pos_test.shape[0],1))\n",
        "neg_label_test = np.zeros((neg_test.shape[0],1))\n",
        "y_test = np.concatenate((pos_label_test, neg_label_test), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32b23398-e80e-4624-b89e-c02fabfd3f8d",
      "metadata": {
        "id": "32b23398-e80e-4624-b89e-c02fabfd3f8d"
      },
      "source": [
        "#### **Data preprocessing**\n",
        "We will do some data preprocessing before we tokenize the data. We will remove `#` symbol, hyperlinks, stop words & punctuations from the data. You can use the `re` package in python to find and replace these strings. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f89d9d69-1640-4583-a7b7-7ec04ccf3310",
      "metadata": {
        "id": "f89d9d69-1640-4583-a7b7-7ec04ccf3310"
      },
      "source": [
        "**1.4) Replace the `#` symbol with '' in every tweet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5db4dd6d-e775-49d3-96e1-57620c042d46",
      "metadata": {
        "id": "5db4dd6d-e775-49d3-96e1-57620c042d46"
      },
      "outputs": [],
      "source": [
        "#code here\n",
        "import re\n",
        "\n",
        "## dev\n",
        "for i in range(X_dev.shape[0]):\n",
        "    sentence = X_dev[i][:]\n",
        "    X_dev[i] =re.sub('#', '', sentence)\n",
        "    \n",
        "## test\n",
        "for i in range(X_test.shape[0]):\n",
        "    sentence = X_test[i][:]\n",
        "    X_test[i] = re.sub('#', '', sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24c4caa8-d71d-46a8-8859-a8e85c56acfe",
      "metadata": {
        "id": "24c4caa8-d71d-46a8-8859-a8e85c56acfe"
      },
      "source": [
        "**1.5) Replace hyperlinks with '' in every tweet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff5a7411-df49-427b-adef-5e8e63224db0",
      "metadata": {
        "id": "ff5a7411-df49-427b-adef-5e8e63224db0"
      },
      "outputs": [],
      "source": [
        "#code here\n",
        "regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"      \n",
        "    \n",
        "## dev\n",
        "for i in range(X_dev.shape[0]):\n",
        "    sentence = X_dev[i][:]\n",
        "    X_dev[i] = re.sub(regex, '', sentence)\n",
        "    \n",
        "## test\n",
        "for i in range(X_test.shape[0]):\n",
        "    sentence = X_test[i][:]\n",
        "    X_test[i] = re.sub(regex, '', sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "492ae463-b611-4292-9ad2-b778856bf8bc",
      "metadata": {
        "id": "492ae463-b611-4292-9ad2-b778856bf8bc"
      },
      "source": [
        "**1.6) Remove all stop words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbae47b3",
      "metadata": {
        "id": "fbae47b3",
        "outputId": "390f3e9d-3240-4db4-a075-bbc32bc9624e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\cwp94\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a805d426",
      "metadata": {
        "id": "a805d426"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "## dev\n",
        "for i in range(X_dev.shape[0]):\n",
        "    sentence = X_dev[i][:]\n",
        "    \n",
        "    word_tokens = sentence.split()\n",
        "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "    X_dev[i] = ' '.join(filtered_sentence)\n",
        "    \n",
        "## test\n",
        "for i in range(X_test.shape[0]):\n",
        "    sentence = X_test[i][:]\n",
        "    word_tokens = sentence.split()\n",
        "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "    X_test[i] = ' '.join(filtered_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "169bf8ad-f7ba-4e67-a1a0-92fcdd193ab9",
      "metadata": {
        "id": "169bf8ad-f7ba-4e67-a1a0-92fcdd193ab9"
      },
      "source": [
        "**1.7) Remove all punctuations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "774743e0-8cf0-4dbb-a6fa-006ff076bb9e",
      "metadata": {
        "id": "774743e0-8cf0-4dbb-a6fa-006ff076bb9e"
      },
      "outputs": [],
      "source": [
        "## dev\n",
        "for i in range(X_dev.shape[0]):\n",
        "    sentence = X_dev[i][:]\n",
        "    X_dev[i] = re.sub(r'[^\\w\\s]', '', sentence)\n",
        "    \n",
        "## test\n",
        "for i in range(X_test.shape[0]):\n",
        "    sentence = X_test[i][:]\n",
        "    X_test[i] = re.sub(r'[^\\w\\s]', '', sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f1af18-0c07-4ffb-994e-daead4740a53",
      "metadata": {
        "id": "b2f1af18-0c07-4ffb-994e-daead4740a53"
      },
      "source": [
        "**1.8) Apply stemming on the development & test datasets using Porter algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96083f85",
      "metadata": {
        "id": "96083f85",
        "outputId": "e9396528-21b3-4172-bf56-ca683a0c299a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\cwp94\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c84a52f6-a62a-4033-8d1d-239ff6904248",
      "metadata": {
        "id": "c84a52f6-a62a-4033-8d1d-239ff6904248"
      },
      "outputs": [],
      "source": [
        "#code here\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def stemSentence(text):\n",
        "    token_words = word_tokenize(text)\n",
        "    stem_sentence = [porter.stem(word) for word in token_words]\n",
        "    return \" \".join(stem_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8f3fc6b",
      "metadata": {
        "id": "b8f3fc6b"
      },
      "outputs": [],
      "source": [
        "X_dev_clean = [stemSentence(sen) for sen in X_dev]\n",
        "X_test_clean = [stemSentence(sen) for sen in X_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "687e23ef-dafd-4183-b2f1-86089e281dd8",
      "metadata": {
        "id": "687e23ef-dafd-4183-b2f1-86089e281dd8"
      },
      "source": [
        "#### **Model training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8126bf33",
      "metadata": {
        "id": "8126bf33"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c40fa44-01ad-4788-98b9-9c8f0c1252ef",
      "metadata": {
        "id": "0c40fa44-01ad-4788-98b9-9c8f0c1252ef"
      },
      "source": [
        "**1.9) Create bag of words features for each tweet in the development dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c17c6b99-9dfb-4d30-9e03-d596a9da880a",
      "metadata": {
        "id": "c17c6b99-9dfb-4d30-9e03-d596a9da880a"
      },
      "outputs": [],
      "source": [
        "#code here\n",
        "vector_bow = CountVectorizer()\n",
        "X_dev_bow = vector_bow.fit_transform(X_dev_clean)\n",
        "X_test_bow = vector_bow.transform(X_test_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4baf65cd-019b-4ff4-b93c-3ca8cfffca8e",
      "metadata": {
        "id": "4baf65cd-019b-4ff4-b93c-3ca8cfffca8e"
      },
      "source": [
        "**1.10) Train a supervised learning model of choice on the development dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3433a6b0-408d-462e-9072-3495b21bc97b",
      "metadata": {
        "id": "3433a6b0-408d-462e-9072-3495b21bc97b",
        "outputId": "ac920d92-e363-4a3c-e0f8-ab807678c6db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#code here\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "lr_bow = LogisticRegression()\n",
        "lr_bow.fit(X_dev_bow, y_dev.flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c16c6f6-7ab2-4d7a-b9dc-098a72381340",
      "metadata": {
        "id": "1c16c6f6-7ab2-4d7a-b9dc-098a72381340"
      },
      "source": [
        "**1.11) Create TF-IDF features for each tweet in the development dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b417843-ffc4-4614-b2ef-964f8ec3e510",
      "metadata": {
        "id": "7b417843-ffc4-4614-b2ef-964f8ec3e510"
      },
      "outputs": [],
      "source": [
        "#code here\n",
        "\n",
        "vector_tfidf = TfidfVectorizer()\n",
        "X_dev_tfidf = vector_tfidf.fit_transform(X_dev_clean)\n",
        "X_test_tfidf = vector_tfidf.transform(X_test_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea3c9776-aad9-4eda-b3c2-d9f6b3e52427",
      "metadata": {
        "id": "ea3c9776-aad9-4eda-b3c2-d9f6b3e52427"
      },
      "source": [
        "**1.12) Train the same supervised learning algorithm on the development dataset with TF-IDF features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8c7fe8b-61de-4daa-a338-74295a4902ce",
      "metadata": {
        "id": "b8c7fe8b-61de-4daa-a338-74295a4902ce",
        "outputId": "cc6f4f49-a6e3-49b9-9159-4b5b4b1b24b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#code here\n",
        "lr_tfidf = LogisticRegression()\n",
        "lr_tfidf.fit(X_dev_tfidf, y_dev.flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab0129e7-a0ea-473e-9ad1-667b44a13a92",
      "metadata": {
        "id": "ab0129e7-a0ea-473e-9ad1-667b44a13a92"
      },
      "source": [
        "**1.13) Compare the performance of the two models on the test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a64ca176-dab8-4965-a85d-dcf9dc013717",
      "metadata": {
        "id": "a64ca176-dab8-4965-a85d-dcf9dc013717",
        "outputId": "6b20f0e0-0ff0-41a3-f8ec-1d18294da1f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*Result from BoW Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.74      0.77      0.76      1000\n",
            "         1.0       0.76      0.73      0.75      1000\n",
            "\n",
            "    accuracy                           0.75      2000\n",
            "   macro avg       0.75      0.75      0.75      2000\n",
            "weighted avg       0.75      0.75      0.75      2000\n",
            "\n",
            "\n",
            "*Result from TF-IDF Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.76      0.76      1000\n",
            "         1.0       0.76      0.76      0.76      1000\n",
            "\n",
            "    accuracy                           0.76      2000\n",
            "   macro avg       0.76      0.76      0.76      2000\n",
            "weighted avg       0.76      0.76      0.76      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#code here\n",
        "print(\"*Result from BoW Logistic Regression\")\n",
        "predictions_bow = lr_bow.predict(X_test_bow)\n",
        "print(classification_report(y_test.flatten(), predictions_bow))\n",
        "print()\n",
        "print(\"*Result from TF-IDF Logistic Regression\")\n",
        "predictions_tfidf = lr_tfidf.predict(X_test_tfidf)\n",
        "print(classification_report(y_test.flatten(), predictions_tfidf))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The overall performance between the two models are very similar. Keeping in mind that the dataset was balanced between the positive and negative tweets, our most important metric will be accuracy. We can see that the accuracy for TF-IDF is slightly higher by 0.01, so we can conclude that TF-IDF Logistic Regression is a better model than BoW Logistic Regression."
      ],
      "metadata": {
        "id": "w3ifeYPE_VJL"
      },
      "id": "w3ifeYPE_VJL"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "af708boepDWH"
      },
      "id": "af708boepDWH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Applied_ML_HW5_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}